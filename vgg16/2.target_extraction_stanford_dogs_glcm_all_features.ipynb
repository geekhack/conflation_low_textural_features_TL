{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (4.6.0.66)\n",
      "Requirement already satisfied: numpy>=1.13.3; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from opencv-python) (1.19.5)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.6/dist-packages (4.6.0.66)\n",
      "Requirement already satisfied: numpy>=1.13.3; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from opencv-python-headless) (1.19.5)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "!pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-image\n",
      "  Downloading scikit_image-0.17.2-cp36-cp36m-manylinux1_x86_64.whl (12.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.4 MB 15.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (2.9.0)\n",
      "Collecting networkx>=2.0\n",
      "  Downloading networkx-2.5.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 28.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (1.5.4)\n",
      "Collecting tifffile>=2019.7.26\n",
      "  Downloading tifffile-2020.9.3-py3-none-any.whl (148 kB)\n",
      "\u001b[K     |███████████████████████████████ | 143 kB 26.8 MB/s eta 0:00:01     |████████████████████████████████| 148 kB 26.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (3.3.3)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.15.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (1.19.5)\n",
      "Collecting PyWavelets>=1.1.1\n",
      "  Downloading PyWavelets-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 40.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (8.1.0)\n",
      "Requirement already satisfied, skipping upgrade: decorator<5,>=4.3 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image) (4.4.2)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.3.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.15.0)\n",
      "Installing collected packages: networkx, tifffile, PyWavelets, scikit-image\n",
      "Successfully installed PyWavelets-1.1.1 networkx-2.5.1 scikit-image-0.17.2 tifffile-2020.9.3\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import Model\n",
    "from keras import layers, Sequential\n",
    "from matplotlib import pyplot\n",
    "from math import floor\n",
    "import numpy as np\n",
    "import cv2\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from numpy import expand_dims, array, exp, max\n",
    "import json\n",
    "import skimage.feature as feature\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 75, 75, 64)        0         \n",
      "=================================================================\n",
      "Total params: 1,792\n",
      "Trainable params: 1,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = VGG16()\n",
    "model2 = Model(inputs=model1.inputs, outputs=model1.layers[1].output)\n",
    "# model.add(layers.MaxPool2D(pool_size=(2, 2),strides=(1, 1), padding='valid'))\n",
    "model = Sequential(layers=model2.layers)\n",
    "model.add(layers.MaxPool2D(pool_size=(3, 3), strides=(3, 3), padding='same'))\n",
    "# model = Model(inputs=model.inputs, outputs = model.layers[1].output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalBinaryPatterns:\n",
    "  def __init__(self, numPoints, radius):\n",
    "    self.numPoints = numPoints\n",
    "    self.radius = radius\n",
    "\n",
    "  def describe(self, image, eps = 1e-7):\n",
    "    lbp = feature.local_binary_pattern(image, self.numPoints, self.radius, method=\"uniform\")\n",
    "    (hist, _) = np.histogram(lbp.ravel(), bins=np.arange(0, self.numPoints+3), range=(0, self.numPoints + 2))\n",
    "\n",
    "    # Normalize the histogram\n",
    "    hist = hist.astype('float')\n",
    "    hist /= (hist.sum() + eps)\n",
    "\n",
    "    return hist, lbp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n02087046_4809.jpg', 'n02087046_3953.jpg', 'n02087046_5860.jpg', 'n02087046_2485.jpg', 'n02087046_5224.jpg', 'n02087046_5626.jpg', 'n02087046_8471.jpg', 'n02087046_4455.jpg', 'n02087046_4506.jpg', 'n02087046_2957.jpg', 'n02087046_406.jpg', 'n02087046_133.jpg', 'n02087046_6857.jpg', 'n02087046_336.jpg', 'n02087046_2276.jpg', 'n02087046_4751.jpg', 'n02087046_5499.jpg', 'n02087046_2752.jpg', 'n02087046_309.jpg', 'n02087046_1520.jpg', 'n02087046_3944.jpg', 'n02087046_4066.jpg', 'n02087046_4235.jpg', 'n02087046_224.jpg', 'n02087046_7166.jpg', 'n02087046_5701.jpg', 'n02087046_1468.jpg', 'n02087046_1724.jpg', 'n02087046_5541.jpg', 'n02087046_4370.jpg', 'n02087046_4127.jpg', 'n02087046_1703.jpg', 'n02087046_2419.jpg', 'n02087046_3543.jpg', 'n02087046_6166.jpg', 'n02087046_5890.jpg', 'n02087046_1577.jpg', 'n02087046_2819.jpg', 'n02087046_2168.jpg', 'n02087046_178.jpg', 'n02087046_568.jpg', 'n02087046_5383.jpg', 'n02087046_4102.jpg', 'n02087046_4039.jpg', 'n02087046_2141.jpg', 'n02087046_1004.jpg', 'n02087046_4647.jpg', 'n02087046_357.jpg', 'n02087046_4808.jpg', 'n02087046_3211.jpg', 'n02087046_2744.jpg', 'n02087046_2127.jpg', 'n02087046_4614.jpg', 'n02087046_6357.jpg', 'n02087046_2969.jpg', 'n02087046_3103.jpg', 'n02087046_420.jpg', 'n02087046_2279.jpg', 'n02087046_1206.jpg', 'n02087046_217.jpg', 'n02087046_5756.jpg', 'n02087046_5840.jpg', 'n02087046_1666.jpg', 'n02087046_3471.jpg', 'n02087046_2614.jpg', 'n02087046_2315.jpg', 'n02087046_4987.jpg', 'n02087046_3724.jpg', 'n02087046_5838.jpg', 'n02087046_4155.jpg', 'n02087046_6986.jpg', 'n02087046_5849.jpg', 'n02087046_2843.jpg', 'n02087046_505.jpg', 'n02087046_2185.jpg', 'n02087046_4426.jpg', 'n02087046_3608.jpg', 'n02087046_1989.jpg', 'n02087046_4135.jpg', 'n02087046_430.jpg', 'n02087046_8379.jpg', 'n02087046_5847.jpg', 'n02087046_7960.jpg', 'n02087046_2193.jpg', 'n02087046_4001.jpg', 'n02087046_6266.jpg', 'n02087046_7037.jpg', 'n02087046_7149.jpg', 'n02087046_924.jpg', 'n02087046_7214.jpg', 'n02087046_3560.jpg', 'n02087046_2316.jpg', 'n02087046_9864.jpg', 'n02087046_6152.jpg', 'n02087046_1386.jpg', 'n02087046_7469.jpg', 'n02087046_6179.jpg', 'n02087046_4575.jpg', 'n02087046_81.jpg', 'n02087046_7015.jpg', 'n02087046_4129.jpg', 'n02087046_5887.jpg', 'n02087046_5439.jpg', 'n02087046_7191.jpg', 'n02087046_3923.jpg', 'n02087046_6029.jpg', 'n02087046_4315.jpg', 'n02087046_7204.jpg', 'n02087046_5305.jpg', 'n02087046_5347.jpg', 'n02087046_4832.jpg', 'n02087046_1792.jpg', 'n02087046_693.jpg', 'n02087046_6211.jpg', 'n02087046_2058.jpg', 'n02087046_6443.jpg', 'n02087046_2140.jpg', 'n02087046_7293.jpg', 'n02087046_5661.jpg', 'n02087046_6476.jpg', 'n02087046_4058.jpg', 'n02087046_1658.jpg', 'n02087046_5219.jpg', 'n02087046_7400.jpg', 'n02087046_267.jpg', 'n02087046_6812.jpg', 'n02087046_7492.jpg', 'n02087046_833.jpg', 'n02087046_3490.jpg', 'n02087046_4409.jpg', 'n02087046_2158.jpg', 'n02087046_6370.jpg', 'n02087046_6902.jpg', 'n02087046_5407.jpg', 'n02087046_4346.jpg', 'n02087046_6003.jpg', 'n02087046_4851.jpg', 'n02087046_3470.jpg', 'n02087046_2152.jpg', 'n02087046_5017.jpg', 'n02087046_7365.jpg', 'n02087046_6205.jpg', 'n02087046_8206.jpg', 'n02087046_4350.jpg', 'n02087046_5196.jpg', 'n02087046_5325.jpg', 'n02087046_4906.jpg', 'n02087046_6513.jpg', 'n02087046_3311.jpg', 'n02087046_296.jpg', 'n02087046_5386.jpg', 'n02087046_6324.jpg', 'n02087046_3651.jpg', 'n02087046_4402.jpg', 'n02087046_4338.jpg', 'n02087046_7245.jpg', 'n02087046_4856.jpg', 'n02087046_5843.jpg', 'n02087046_5203.jpg', 'n02087046_4394.jpg', 'n02087046_3462.jpg', 'n02087046_3735.jpg', 'n02087046_2551.jpg', 'n02087046_3843.jpg', 'n02087046_1482.jpg', 'n02087046_228.jpg', 'n02087046_2199.jpg', 'n02087046_538.jpg', 'n02087046_4401.jpg', 'n02087046_6522.jpg', 'n02087046_5306.jpg', 'n02087046_3439.jpg']\n"
     ]
    }
   ],
   "source": [
    "###########################THIS CODE CHECKS IMAGES FROM A FOLDER#################################\n",
    "\n",
    "def sortTrainImages():\n",
    "    # get the class labels from training datasets\n",
    "    p = {}\n",
    "    data_path = '../StanfordDogs/Stanford_Dogs'\n",
    "    img_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1. / 255, rotation_range=20)\n",
    "    #labels = img_gen.flow_from_directory(data_path + '/Images')\n",
    "    #train_labels = labels.class_indices.keys()\n",
    "\n",
    "    #for lbl in train_labels:\n",
    "    resnet_path = data_path +'/Images/' + 'n02087046-toy_terrier'\n",
    "    p_files = [f for f in listdir(resnet_path) if isfile(join(resnet_path, f))]\n",
    "    pests_images = np.empty(len(p_files), dtype=object)\n",
    "    print(p_files)\n",
    "    for m in range(0, len(p_files)):\n",
    "        \n",
    "            #print(join(resnet_path, p_files[m]))\n",
    "        imageName_x = p_files[m]\n",
    "        p.update({'n02087046-toy_terrier' + \"_\" + str(m): imageName_x})\n",
    "            ######if m < 25:\n",
    "                #pests_images[m] = cv2.imread(join(resnet_path, p_files[m]))\n",
    "                # get the name of the image\n",
    "                ######imageName_x = p_files[m]\n",
    "                #####p.update({lbl + \"_\" + str(m): imageName_x})\n",
    "\n",
    "    return p\n",
    "\n",
    "\n",
    "images_collection = sortTrainImages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softMax(temperature, features_list):\n",
    "    feat_list = [x / temperature for x in features_list]\n",
    "    feature_list = exp(feat_list - max(feat_list))\n",
    "    return feature_list / feature_list.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n02087046-toy_terrier_0\n",
      "n02087046-toy_terrier_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n02087046-toy_terrier_2\n",
      "n02087046-toy_terrier_3\n",
      "n02087046-toy_terrier_4\n",
      "n02087046-toy_terrier_5\n",
      "n02087046-toy_terrier_6\n",
      "n02087046-toy_terrier_7\n",
      "n02087046-toy_terrier_8\n",
      "n02087046-toy_terrier_9\n",
      "n02087046-toy_terrier_10\n",
      "n02087046-toy_terrier_11\n",
      "n02087046-toy_terrier_12\n",
      "n02087046-toy_terrier_13\n",
      "n02087046-toy_terrier_14\n",
      "n02087046-toy_terrier_15\n",
      "n02087046-toy_terrier_16\n",
      "n02087046-toy_terrier_17\n",
      "n02087046-toy_terrier_18\n",
      "n02087046-toy_terrier_19\n",
      "n02087046-toy_terrier_20\n",
      "n02087046-toy_terrier_21\n",
      "n02087046-toy_terrier_22\n",
      "n02087046-toy_terrier_23\n",
      "n02087046-toy_terrier_24\n",
      "n02087046-toy_terrier_25\n",
      "n02087046-toy_terrier_26\n",
      "n02087046-toy_terrier_27\n",
      "n02087046-toy_terrier_28\n",
      "n02087046-toy_terrier_29\n",
      "n02087046-toy_terrier_30\n",
      "n02087046-toy_terrier_31\n",
      "n02087046-toy_terrier_32\n",
      "n02087046-toy_terrier_33\n",
      "n02087046-toy_terrier_34\n",
      "n02087046-toy_terrier_35\n",
      "n02087046-toy_terrier_36\n",
      "n02087046-toy_terrier_37\n",
      "n02087046-toy_terrier_38\n",
      "n02087046-toy_terrier_39\n",
      "n02087046-toy_terrier_40\n",
      "n02087046-toy_terrier_41\n",
      "n02087046-toy_terrier_42\n",
      "n02087046-toy_terrier_43\n",
      "n02087046-toy_terrier_44\n",
      "n02087046-toy_terrier_45\n",
      "n02087046-toy_terrier_46\n",
      "n02087046-toy_terrier_47\n",
      "n02087046-toy_terrier_48\n",
      "n02087046-toy_terrier_49\n",
      "n02087046-toy_terrier_50\n",
      "n02087046-toy_terrier_51\n",
      "n02087046-toy_terrier_52\n",
      "n02087046-toy_terrier_53\n",
      "n02087046-toy_terrier_54\n",
      "n02087046-toy_terrier_55\n",
      "n02087046-toy_terrier_56\n",
      "n02087046-toy_terrier_57\n",
      "n02087046-toy_terrier_58\n",
      "n02087046-toy_terrier_59\n",
      "n02087046-toy_terrier_60\n",
      "n02087046-toy_terrier_61\n",
      "n02087046-toy_terrier_62\n",
      "n02087046-toy_terrier_63\n",
      "n02087046-toy_terrier_64\n",
      "n02087046-toy_terrier_65\n",
      "n02087046-toy_terrier_66\n",
      "n02087046-toy_terrier_67\n",
      "n02087046-toy_terrier_68\n",
      "n02087046-toy_terrier_69\n",
      "n02087046-toy_terrier_70\n",
      "n02087046-toy_terrier_71\n",
      "n02087046-toy_terrier_72\n",
      "n02087046-toy_terrier_73\n",
      "n02087046-toy_terrier_74\n",
      "n02087046-toy_terrier_75\n",
      "n02087046-toy_terrier_76\n",
      "n02087046-toy_terrier_77\n",
      "n02087046-toy_terrier_78\n",
      "n02087046-toy_terrier_79\n",
      "n02087046-toy_terrier_80\n",
      "n02087046-toy_terrier_81\n",
      "n02087046-toy_terrier_82\n",
      "n02087046-toy_terrier_83\n",
      "n02087046-toy_terrier_84\n",
      "n02087046-toy_terrier_85\n",
      "n02087046-toy_terrier_86\n",
      "n02087046-toy_terrier_87\n",
      "n02087046-toy_terrier_88\n",
      "n02087046-toy_terrier_89\n",
      "n02087046-toy_terrier_90\n",
      "n02087046-toy_terrier_91\n",
      "n02087046-toy_terrier_92\n",
      "n02087046-toy_terrier_93\n",
      "n02087046-toy_terrier_94\n",
      "n02087046-toy_terrier_95\n",
      "n02087046-toy_terrier_96\n",
      "n02087046-toy_terrier_97\n",
      "n02087046-toy_terrier_98\n",
      "n02087046-toy_terrier_99\n",
      "n02087046-toy_terrier_100\n",
      "n02087046-toy_terrier_101\n",
      "n02087046-toy_terrier_102\n",
      "n02087046-toy_terrier_103\n",
      "n02087046-toy_terrier_104\n",
      "n02087046-toy_terrier_105\n",
      "n02087046-toy_terrier_106\n",
      "n02087046-toy_terrier_107\n",
      "n02087046-toy_terrier_108\n",
      "n02087046-toy_terrier_109\n",
      "n02087046-toy_terrier_110\n",
      "n02087046-toy_terrier_111\n",
      "n02087046-toy_terrier_112\n",
      "n02087046-toy_terrier_113\n",
      "n02087046-toy_terrier_114\n",
      "n02087046-toy_terrier_115\n",
      "n02087046-toy_terrier_116\n",
      "n02087046-toy_terrier_117\n",
      "n02087046-toy_terrier_118\n",
      "n02087046-toy_terrier_119\n",
      "n02087046-toy_terrier_120\n",
      "n02087046-toy_terrier_121\n",
      "n02087046-toy_terrier_122\n",
      "n02087046-toy_terrier_123\n",
      "n02087046-toy_terrier_124\n",
      "n02087046-toy_terrier_125\n",
      "n02087046-toy_terrier_126\n",
      "n02087046-toy_terrier_127\n",
      "n02087046-toy_terrier_128\n",
      "n02087046-toy_terrier_129\n",
      "n02087046-toy_terrier_130\n",
      "n02087046-toy_terrier_131\n",
      "n02087046-toy_terrier_132\n",
      "n02087046-toy_terrier_133\n",
      "n02087046-toy_terrier_134\n",
      "n02087046-toy_terrier_135\n",
      "n02087046-toy_terrier_136\n",
      "n02087046-toy_terrier_137\n",
      "n02087046-toy_terrier_138\n",
      "n02087046-toy_terrier_139\n",
      "n02087046-toy_terrier_140\n",
      "n02087046-toy_terrier_141\n",
      "n02087046-toy_terrier_142\n",
      "n02087046-toy_terrier_143\n",
      "n02087046-toy_terrier_144\n",
      "n02087046-toy_terrier_145\n",
      "n02087046-toy_terrier_146\n",
      "n02087046-toy_terrier_147\n",
      "n02087046-toy_terrier_148\n",
      "n02087046-toy_terrier_149\n",
      "n02087046-toy_terrier_150\n",
      "n02087046-toy_terrier_151\n",
      "n02087046-toy_terrier_152\n",
      "n02087046-toy_terrier_153\n",
      "n02087046-toy_terrier_154\n",
      "n02087046-toy_terrier_155\n",
      "n02087046-toy_terrier_156\n",
      "n02087046-toy_terrier_157\n",
      "n02087046-toy_terrier_158\n",
      "n02087046-toy_terrier_159\n",
      "n02087046-toy_terrier_160\n",
      "n02087046-toy_terrier_161\n",
      "n02087046-toy_terrier_162\n",
      "n02087046-toy_terrier_163\n",
      "n02087046-toy_terrier_164\n",
      "n02087046-toy_terrier_165\n",
      "n02087046-toy_terrier_166\n",
      "n02087046-toy_terrier_167\n",
      "n02087046-toy_terrier_168\n",
      "n02087046-toy_terrier_169\n",
      "n02087046-toy_terrier_170\n",
      "n02087046-toy_terrier_171\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAADrCAYAAABXYUzjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAENklEQVR4nO3cwW3jMABFQXGREpxz2H8tdhE5Jz1wG4gBO5D3CdqZMyEQ+MA78KCx1toA+Pf+1BcA+F8JMEBEgAEiAgwQEWCAiAADRN6eOXy5XNac80VX4VG32+17rfW+1/fsegx2Pa972z4V4Dnndr1e97sVvzLG+Nzze3Y9Brue171tPUEARAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiY631+OExvrZt+3zddXjQx1rrfa+P2fUw7HpeP277VIAB2I8nCICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBgg8vbM4cvlsuacL7oKj7rdbt97/rTFrsdg1/O6t+1TAZ5zbtfrdb9b8StjjF3/cGXXY7Dred3b1hMEQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggMtZajx8e42vbts/XXYcHfay13vf6mF0Pw67n9eO2TwUYgP14ggCICDBARIABIgIMEBFggIgAA0QEGCAiwAARAQaICDBA5O2Zw5fLZc05X3QVHnW73b73/GeAXY/Brud1b9unAjzn3K7X63634lfGGLv+YMWux2DX87q3rScIgIgAA0QEGCAiwAARAQaICDBARIABIgIMEBFggIgAA0QEGCAiwAARAQaICDBARIABIgIMEBFggIgAA0QEGCAiwAARAQaICDBARIABIgIMEBFggIgAA0QEGCAiwAARAQaICDBARIABIgIMEBFggIgAA0QEGCAiwAARAQaICDBARIABIgIMEBFggIgAA0QEGCAiwAARAQaICDBARIABIgIMEBFggIgAA0QEGCAiwAARAQaICDBARIABIgIMEBFggIgAA0QEGCAiwAARAQaICDBAZKy1Hj88xte2bZ+vuw4P+lhrve/1Mbsehl3P68dtnwowAPvxBAEQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQ+QtCyayvdbrBBgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features_prob_dist_array = []\n",
    "\n",
    "for o, j in images_collection.items():\n",
    "    print(o)\n",
    "    contrast_features_array = []\n",
    "    homogeneity_features_array = []\n",
    "    dissimilarity_features_array = []\n",
    "    energy_features_array = []\n",
    "    correlation_features_array = []\n",
    "    \n",
    "    image_string = '../StanfordDogs/Stanford_Dogs/Images/n02087046-toy_terrier/' + j\n",
    "  \n",
    "    img = imread(image_string)\n",
    "    img = resize(img,(224,224,3))\n",
    "        # get the path of image to extract the class\n",
    "    img_class =j[0]\n",
    "    img = img_to_array(img)    \n",
    "    # expand the image dimensions\n",
    "    img = expand_dims(img, axis=0)\n",
    "        # scale the pixels\n",
    "    img = preprocess_input(img)\n",
    "        # get the features of the first layer\n",
    "    feature_maps = model.predict(img)\n",
    "    \n",
    "    correlation_numerator = []\n",
    "    energy_numerator = []\n",
    "    homogeneity_numerator = []\n",
    "    dissimilarity_numerator = []\n",
    "    contrast_numerator = []\n",
    "    \n",
    "    ix = 1\n",
    "    square = 3\n",
    "\n",
    "    for _ in range(square):\n",
    "        for _ in range(square):\n",
    "            ax = pyplot.subplot(square, square, ix)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ##################FLATTEN THE FEATURE for dimensionality reduction##################################\n",
    "            feature_items = array(feature_maps[0, :, :, ix - 1])\n",
    "            #get the unsigned values of the feature\n",
    "            features_x = feature_items.astype(np.uint8)        \n",
    "            #get the grey-level cooccurrence matrix\n",
    "            graycom = feature.greycomatrix(features_x, [1], [0, np.pi/4, np.pi/2, 3*np.pi/4], levels=256)            \n",
    "            # Find the GLCM properties\n",
    "            correlation = feature.greycoprops(graycom, 'correlation') \n",
    "            contrast = feature.greycoprops(graycom, 'contrast')\n",
    "            dissimilarity = feature.greycoprops(graycom, 'dissimilarity')\n",
    "            homogeneity = feature.greycoprops(graycom, 'homogeneity')\n",
    "            energy = feature.greycoprops(graycom, 'energy')\n",
    "            correlation = feature.greycoprops(graycom, 'correlation') \n",
    "            \n",
    "            #get the probability distribution using correlation harrick features except correlation\n",
    "            contrast_prob = softMax(0.5,contrast[0])\n",
    "            dissimilarity_prob = softMax(0.5,dissimilarity[0])\n",
    "            homogeneity_prob = softMax(0.5,homogeneity[0])\n",
    "            energy_prob = softMax(0.5,energy[0])\n",
    "            correlation_prob = softMax(0.5,correlation[0])\n",
    "            \n",
    "            #get the conflated distribution for this feature\n",
    "            conf_numerator_contrast = np.prod(contrast_prob)\n",
    "            conf_numerator_dissimilarity = np.prod(correlation_prob)\n",
    "            conf_numerator_homogeneity = np.prod(homogeneity_prob)\n",
    "            conf_numerator_energy = np.prod(energy_prob)\n",
    "            conf_numerator_correlation = np.prod(correlation_prob)\n",
    "            \n",
    "            contrast_numerator.append(conf_numerator_contrast)\n",
    "            energy_numerator.append(conf_numerator_energy)\n",
    "            dissimilarity_numerator.append(conf_numerator_dissimilarity)\n",
    "            homogeneity_numerator.append(conf_numerator_homogeneity)\n",
    "            correlation_numerator.append(conf_numerator_correlation)\n",
    "            \n",
    "            #image_features_array.append(correlation_prob)\n",
    "            contrast_features_array.append(contrast_numerator)\n",
    "            homogeneity_features_array.append(homogeneity_numerator)\n",
    "            dissimilarity_features_array.append(dissimilarity_numerator)\n",
    "            energy_features_array.append(energy_numerator)\n",
    "            correlation_features_array.append(correlation_numerator)\n",
    "            #pyplot.imshow(feature_maps[0, :, :, ix - 1], cmap='gray')\n",
    "            \n",
    "            ix += 1\n",
    "    correlation_denominator = np.trapz(correlation_numerator,axis=0)\n",
    "    contrast_denominator = np.trapz(contrast_numerator,axis=0)\n",
    "    homogeneity_denominator = np.trapz(homogeneity_numerator,axis=0)\n",
    "    dissimilarity_denominator = np.trapz(dissimilarity_numerator,axis=0)\n",
    "    energy_denominator = np.trapz(energy_numerator,axis=0)\n",
    "    \n",
    "    #conflated distribution for the image becomes\n",
    "    conflated_dist_correlation = correlation_numerator/correlation_denominator\n",
    "    conflated_dist_contrast = contrast_numerator/contrast_denominator\n",
    "    conflated_dist_homogeneity = homogeneity_numerator/homogeneity_denominator\n",
    "    conflated_dist_dissimilarity = dissimilarity_numerator/dissimilarity_denominator\n",
    "    conflated_dist_energy = energy_numerator/energy_denominator\n",
    "    \n",
    "    new_rec_corr = {j + \"_/\" +'n02087046-toy_terrier': conflated_dist_correlation.tolist()}\n",
    "    new_rec_cont = {j + \"_/\" +'n02087046-toy_terrier': conflated_dist_contrast.tolist()}\n",
    "    new_rec_homo = {j + \"_/\" +'n02087046-toy_terrier': conflated_dist_homogeneity.tolist()}\n",
    "    new_rec_diss = {j + \"_/\" +'n02087046-toy_terrier': conflated_dist_dissimilarity.tolist()}\n",
    "    new_rec_ener = {j + \"_/\" +'n02087046-toy_terrier': conflated_dist_energy.tolist()}\n",
    "        \n",
    "    with open(\"../files/stanford/glcm/stanford_domain_data_new_correlation_glcm.json\", 'r+') as file:\n",
    "        # First we load existing data into a dict.\n",
    "        file_data = json.load(file)\n",
    "        # Join new_data with file_data inside emp_details\n",
    "        file_data[\"target_images\"].append(new_rec_corr)\n",
    "        # Sets file's current position at offset.\n",
    "        file.seek(0)\n",
    "        # convert back to json.\n",
    "        json.dump(file_data, file, indent=4)\n",
    "    \n",
    "    with open(\"../files/stanford/glcm/stanford_domain_data_new_contrast_glcm.json\", 'r+') as file:\n",
    "        # First we load existing data into a dict.\n",
    "        file_data = json.load(file)\n",
    "        # Join new_data with file_data inside emp_details\n",
    "        file_data[\"target_images\"].append(new_rec_cont)\n",
    "        # Sets file's current position at offset.\n",
    "        file.seek(0)\n",
    "        # convert back to json.\n",
    "        json.dump(file_data, file, indent=4)\n",
    "    \n",
    "    with open(\"../files/stanford/glcm/stanford_domain_data_new_homogeneity_glcm.json\", 'r+') as file:\n",
    "        # First we load existing data into a dict.\n",
    "        file_data = json.load(file)\n",
    "        # Join new_data with file_data inside emp_details\n",
    "        file_data[\"target_images\"].append(new_rec_homo)\n",
    "        # Sets file's current position at offset.\n",
    "        file.seek(0)\n",
    "        # convert back to json.\n",
    "        json.dump(file_data, file, indent=4)\n",
    "        \n",
    "    with open(\"../files/stanford/glcm/stanford_domain_data_new_dissimilarity_glcm.json\", 'r+') as file:\n",
    "        # First we load existing data into a dict.\n",
    "        file_data = json.load(file)\n",
    "        # Join new_data with file_data inside emp_details\n",
    "        file_data[\"target_images\"].append(new_rec_diss)\n",
    "        # Sets file's current position at offset.\n",
    "        file.seek(0)\n",
    "        # convert back to json.\n",
    "        json.dump(file_data, file, indent=4)\n",
    "        \n",
    "    with open(\"../files/stanford/glcm/stanford_domain_data_new_energy_glcm.json\", 'r+') as file:\n",
    "        # First we load existing data into a dict.\n",
    "        file_data = json.load(file)\n",
    "        # Join new_data with file_data inside emp_details\n",
    "        file_data[\"target_images\"].append(new_rec_ener)\n",
    "        # Sets file's current position at offset.\n",
    "        file.seek(0)\n",
    "        # convert back to json.\n",
    "        json.dump(file_data, file, indent=4)\n",
    "\n",
    "    ##if they need to be saved in a bigger file\n",
    "    #features_prob_dist_array.append(conflated_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
